{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fdcea86-7526-4185-9bd9-593a08249969",
   "metadata": {},
   "source": [
    "# PySpark Basics\n",
    "\n",
    "Welcome to **PySpark Basics**!  \n",
    "This notebook introduces you to **Apache Spark’s Python API (PySpark)** for large-scale data processing.\n",
    "\n",
    "You’ll learn how to:\n",
    "- Initialize a SparkSession  \n",
    "- Create and manipulate DataFrames & RDDs  \n",
    "- Apply transformations and actions  \n",
    "- Perform aggregations and joins  \n",
    "- Use SQL queries on Spark DataFrames  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8f23925-26d4-4a9d-a9dc-ed882135b55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python: C:\\Users\\ksiri\\anaconda3\\envs\\sparkenv\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# ✅ Ensure Spark uses the same Python you're running this notebook with\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "# (Optional) if Spark is installed manually, uncomment and set these:\n",
    "# os.environ[\"SPARK_HOME\"] = \"C:\\\\spark\"\n",
    "# os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n",
    "\n",
    "print(\"Using Python:\", sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1533496-1633-4deb-b0eb-62cafc4b1548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark started successfully!\n",
      "Spark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PySpark without Hadoop binaries\")\n",
    "    .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.file.impl\", \"org.apache.hadoop.fs.local.LocalFs\")\n",
    "    .config(\"spark.hadoop.fs.permissions.umask-mode\", \"000\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"✅ Spark started successfully!\")\n",
    "print(\"Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6db580-be14-4890-8bb4-c97c33cfaeee",
   "metadata": {},
   "source": [
    "## Creating DataFrames\n",
    "\n",
    "A **DataFrame** in PySpark is similar to a Pandas DataFrame — but distributed across a cluster.\n",
    "\n",
    "You can create DataFrames from:\n",
    "1. Python lists or dictionaries  \n",
    "2. External sources like CSV, JSON, Parquet, or databases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1aba2095-2340-4e40-abe5-54240fcdae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------------+\n",
      "|   name|age|         city|\n",
      "+-------+---+-------------+\n",
      "|  Alice| 25|     New York|\n",
      "|    Bob| 30|San Francisco|\n",
      "|Charlie| 35|      Chicago|\n",
      "+-------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(name=\"Alice\", age=25, city=\"New York\"),\n",
    "    Row(name=\"Bob\", age=30, city=\"San Francisco\"),\n",
    "    Row(name=\"Charlie\", age=35, city=\"Chicago\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159d91a-eb79-4a7c-95fc-75194e656058",
   "metadata": {},
   "source": [
    "## Reading Data from CSV/JSON\n",
    "\n",
    "PySpark can read data from multiple file formats.  \n",
    "Here’s how to read a CSV file and infer its schema automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba4e6857-2ed9-42fd-9879-6ccdec71b5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File downloaded successfully!\n",
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv\"\n",
    "local_path = \"tips.csv\"\n",
    "\n",
    "urllib.request.urlretrieve(url, local_path)\n",
    "print(\"✅ File downloaded successfully!\")\n",
    "\n",
    "tips_df = spark.read.csv(local_path, header=True, inferSchema=True)\n",
    "tips_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f4c42-82ea-4067-8903-1e73222b2a39",
   "metadata": {},
   "source": [
    "## DataFrame Operations\n",
    "\n",
    "You can perform filtering, selecting, sorting, and aggregations on DataFrames easily.\n",
    "\n",
    "Common functions:\n",
    "- `select()`\n",
    "- `filter()` / `where()`\n",
    "- `groupBy()`\n",
    "- `orderBy()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbed0b7d-df0b-4ec6-b4b0-84a226853bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----+\n",
      "|   sex|total_bill| tip|\n",
      "+------+----------+----+\n",
      "|Female|     16.99|1.01|\n",
      "|  Male|     10.34|1.66|\n",
      "|  Male|     21.01| 3.5|\n",
      "|  Male|     23.68|3.31|\n",
      "|Female|     24.59|3.61|\n",
      "+------+----------+----+\n",
      "only showing top 5 rows\n",
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     39.42|7.58|  Male|    No|Sat|Dinner|   4|\n",
      "|      30.4| 5.6|  Male|    No|Sun|Dinner|   4|\n",
      "|      32.4| 6.0|  Male|    No|Sun|Dinner|   4|\n",
      "|     34.81| 5.2|Female|    No|Sun|Dinner|   4|\n",
      "|     48.27|6.73|  Male|    No|Sat|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "+----------+----+----+------+---+------+----+\n",
      "|total_bill| tip| sex|smoker|day|  time|size|\n",
      "+----------+----+----+------+---+------+----+\n",
      "|     50.81|10.0|Male|   Yes|Sat|Dinner|   3|\n",
      "|     48.33| 9.0|Male|    No|Sat|Dinner|   4|\n",
      "|     48.27|6.73|Male|    No|Sat|Dinner|   4|\n",
      "|     48.17| 5.0|Male|    No|Sun|Dinner|   6|\n",
      "|     45.35| 3.5|Male|   Yes|Sun|Dinner|   3|\n",
      "+----------+----+----+------+---+------+----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "tips_df.select(\"sex\", \"total_bill\", \"tip\").show(5)\n",
    "\n",
    "# Filter rows where tip > 5\n",
    "tips_df.filter(col(\"tip\") > 5).show(5)\n",
    "\n",
    "# Sort by total_bill\n",
    "tips_df.orderBy(col(\"total_bill\").desc()).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c43b243-b508-46dd-94c1-2a13a016346c",
   "metadata": {},
   "source": [
    "## Aggregations\n",
    "\n",
    "You can group and summarize data using `groupBy()` and aggregate functions like `avg()`, `sum()`, `count()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b22d9d2c-06c0-475d-9e54-830c4cfa83cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-----------------+------------------+\n",
      "| day|total_records|          avg_tip|       total_sales|\n",
      "+----+-------------+-----------------+------------------+\n",
      "|Thur|           62|2.771451612903226|1096.3299999999997|\n",
      "| Sun|           76|3.255131578947369|1627.1600000000003|\n",
      "| Sat|           87|2.993103448275862|1778.3999999999996|\n",
      "| Fri|           19|2.734736842105263|325.87999999999994|\n",
      "+----+-------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, sum, count\n",
    "\n",
    "tips_df.groupBy(\"day\").agg(\n",
    "    count(\"*\").alias(\"total_records\"),\n",
    "    avg(\"tip\").alias(\"avg_tip\"),\n",
    "    sum(\"total_bill\").alias(\"total_sales\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f421c3fb-4727-4284-9c66-a169190184ec",
   "metadata": {},
   "source": [
    "## Adding and Modifying Columns\n",
    "\n",
    "Use `withColumn()` to add or modify columns dynamically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec66a0ea-5049-48de-8224-4b696a8dda17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------------------+\n",
      "|total_bill| tip|       tip_percent|\n",
      "+----------+----+------------------+\n",
      "|     16.99|1.01|5.9446733372572105|\n",
      "|     10.34|1.66|16.054158607350097|\n",
      "|     21.01| 3.5|16.658733936220845|\n",
      "|     23.68|3.31| 13.97804054054054|\n",
      "|     24.59|3.61|14.680764538430255|\n",
      "+----------+----+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Add a new column: tip percentage\n",
    "tips_df = tips_df.withColumn(\"tip_percent\", (col(\"tip\") / col(\"total_bill\")) * 100)\n",
    "tips_df.select(\"total_bill\", \"tip\", \"tip_percent\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f93b08-0cad-4e4b-b641-f9d94787981b",
   "metadata": {},
   "source": [
    "## DataFrame Joins\n",
    "\n",
    "You can join multiple DataFrames on a common column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7a9f38d-88a8-4e73-ae49-5a62a4ac31f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+---+-----+\n",
      "|         city|   name|age|state|\n",
      "+-------------+-------+---+-----+\n",
      "|      Chicago|Charlie| 35|   IL|\n",
      "|     New York|  Alice| 25|   NY|\n",
      "|San Francisco|    Bob| 30|   CA|\n",
      "+-------------+-------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Create another DataFrame for demonstration\n",
    "city_data = [\n",
    "    Row(city=\"New York\", state=\"NY\"),\n",
    "    Row(city=\"Chicago\", state=\"IL\"),\n",
    "    Row(city=\"San Francisco\", state=\"CA\")\n",
    "]\n",
    "city_df = spark.createDataFrame(city_data)\n",
    "\n",
    "# Join with tips_df (pretend it has a city column)\n",
    "joined_df = df.join(city_df, on=\"city\", how=\"inner\")\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd20f117-0a87-415e-b9d5-aaa8bdfc189b",
   "metadata": {},
   "source": [
    "## Using SQL Queries\n",
    "\n",
    "You can register a DataFrame as a temporary SQL table and query it using SQL syntax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e7469df-daf3-422e-9b39-4565e5dd3cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "| day|avg_tip|count|\n",
      "+----+-------+-----+\n",
      "| Sun|   3.26|   76|\n",
      "| Sat|   2.99|   87|\n",
      "|Thur|   2.77|   62|\n",
      "| Fri|   2.73|   19|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrame as SQL table\n",
    "tips_df.createOrReplaceTempView(\"tips\")\n",
    "\n",
    "# Run SQL query\n",
    "sql_result = spark.sql(\"\"\"\n",
    "SELECT day, ROUND(AVG(tip), 2) AS avg_tip, COUNT(*) AS count\n",
    "FROM tips\n",
    "GROUP BY day\n",
    "ORDER BY avg_tip DESC\n",
    "\"\"\")\n",
    "sql_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75b2d7-40f6-41a7-a3b3-918287a4ab98",
   "metadata": {},
   "source": [
    "## RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "Before DataFrames, Spark used **RDDs** — low-level distributed collections.\n",
    "\n",
    "Though DataFrames are preferred now, it’s useful to know basic RDD operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ed81ac9-b2eb-45df-9750-aae8e0784d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD elements: [1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# Transformations and Actions\n",
    "rdd_squared = rdd.map(lambda x: x * x)\n",
    "print(\"RDD elements:\", rdd_squared.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edcd25-58f8-4fc0-b329-f5ca2d1d02ca",
   "metadata": {},
   "source": [
    "## Saving Data\n",
    "\n",
    "You can write Spark DataFrames to local storage or cloud (CSV, Parquet, JSON, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8138f-dc73-4ae3-9f5d-83ab9a747887",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_df.write.mode(\"overwrite\").parquet(\"tips_parquet\")\n",
    "parquet_df = spark.read.parquet(\"tips_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61d0d6-f381-4905-8b05-1f919765070f",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- How to create and manipulate Spark DataFrames  \n",
    "- Basic transformations and actions  \n",
    "- SQL queries on Spark  \n",
    "- Simple joins, aggregations, and file operations  \n",
    "\n",
    "Next Steps:\n",
    "👉 Move to advanced PySpark: window functions, UDFs, optimization, and pipeline building.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99465caf-6928-440c-af34-4f2003db4201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
