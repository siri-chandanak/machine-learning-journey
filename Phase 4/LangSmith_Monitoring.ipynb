{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd5f61b-0bf1-4bfc-926b-34096d3fffec",
   "metadata": {},
   "source": [
    "# LangSmith: Evaluation & Monitoring of Agentic AI\n",
    "\n",
    "This notebook covers the usage of **LangSmith** for evaluating and monitoring agentic AI systems, including **tracking prompts, responses, and performance metrics**.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction to LangSmith\n",
    "2. Why Evaluation & Monitoring Matter\n",
    "3. Setting Up LangSmith\n",
    "4. Tracking Prompts and Responses\n",
    "5. Evaluating Agent Performance\n",
    "6. Best Practices and Notes\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to LangSmith\n",
    "\n",
    "**LangSmith** is a tool for **evaluating and monitoring agentic AI systems**.  \n",
    "It helps you:\n",
    "\n",
    "- Track prompts and responses from AI agents\n",
    "- Evaluate outputs for correctness, relevance, and performance\n",
    "- Monitor agent behavior over time for consistency and improvements\n",
    "\n",
    "LangSmith integrates seamlessly with **LLMs and LangChain agents**.\n",
    " \n",
    "---\n",
    "\n",
    "## 2. Why Evaluation & Monitoring Matter\n",
    "\n",
    "When deploying AI agents in production, it's critical to:\n",
    "\n",
    "1. **Understand agent behavior** – How the agent interprets prompts and executes tasks.\n",
    "2. **Measure performance** – Accuracy, relevance, or success rate of responses.\n",
    "3. **Detect failures or drift** – Monitor changes in agent outputs over time.\n",
    "4. **Optimize prompts and chains** – Improve the agent iteratively using feedback.\n",
    "\n",
    "Without monitoring, agents may produce unexpected or harmful outputs in production.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Setting Up LangSmith\n",
    "\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e7b29d1-ecf9-421f-b143-7abea58c51eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f67e965-fef2-42be-9e8f-36dfebf95f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your LangSmith API key\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"your_key\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df66df2b-a7c2-433b-8d5e-d94b69fae95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.run_helpers import traceable\n",
    "\n",
    "# Initialize the client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35280480-05f8-4d1a-9e07-a6d98bf4921d",
   "metadata": {},
   "source": [
    "## 4. Logging a Run with the `traceable` Decorator\n",
    "\n",
    "You can log a run by decorating a function with `@traceable` and providing **input, output, and optional metadata**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d87d1fd-420e-43f6-bafe-0c2d239c1ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elon Musk'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example prompt and agent response\n",
    "prompt_text = \"Who is the CEO of Tesla?\"\n",
    "response_text = \"Elon Musk\"\n",
    "\n",
    "@traceable(name=\"QA_Agent_Run\", metadata={\"model\": \"GPT-4\", \"task\": \"QA\"})\n",
    "def qa_agent(prompt):\n",
    "    # Simulate agent response\n",
    "    return response_text\n",
    "\n",
    "# Call the function to log the run\n",
    "qa_agent(prompt_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb8118-a3b2-4e7c-8826-28c8526badee",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "- `@traceable`: Decorator to log the function call as a run in LangSmith\n",
    "- `name`: A human-readable name for this run\n",
    "- `metadata`: Optional dictionary with model info, task type, or version\n",
    "- `qa_agent(prompt)`: Function call that triggers the logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eefeb5-48f1-43ff-a764-b06d4c36b042",
   "metadata": {},
   "source": [
    "### Fetching the Latest Run from LangSmith\n",
    "\n",
    "In this step, we connect to our LangSmith project and retrieve recent runs.  \n",
    "Each **run** represents a single tracked execution (for example, a model query or an evaluation).  \n",
    "\n",
    "We specify the `project_name` — here it’s `\"default\"` — and use the `list_runs()` method to fetch the five most recent runs associated with that project.  \n",
    "\n",
    "The code checks whether any runs exist:\n",
    "- If runs are found, it stores the most recent one in `latest_run` and extracts its unique `run_id`.\n",
    "- If no runs are available, it simply prints a message indicating that none were found.\n",
    "\n",
    "This `run_id` can then be used for logging feedback, inspecting results, or linking to the LangSmith dashboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0b915eb-b389-4440-a633-08166af39555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest run ID: 08d3f107-f66e-4ab5-97da-96aed14963ab\n"
     ]
    }
   ],
   "source": [
    "# Replace with your project name in LangSmith\n",
    "project_name = \"default\"  # or your specific project name\n",
    "\n",
    "# list_runs now works\n",
    "recent_runs = list(client.list_runs(project_name=project_name, limit=5))\n",
    "\n",
    "if recent_runs:\n",
    "    latest_run = recent_runs[0]\n",
    "    run_id = latest_run.id\n",
    "    print(\"Latest run ID:\", run_id)\n",
    "else:\n",
    "    print(\"No runs found for this project.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c05bb4-0f9a-41e6-8118-c22b439c5207",
   "metadata": {},
   "source": [
    "## 5. Adding a Rating to a Run\n",
    "\n",
    "You can rate a run to track performance and correctness.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- `run_id`: The ID of the run to rate\n",
    "- `key`: The feedback key (e.g., \"correctness\")\n",
    "- `score`: Numerical score (1–5)\n",
    "- `value`: Optional text feedback\n",
    "- Ratings help track agent **accuracy, relevance, and quality** over time\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "868523c3-ba64-4da2-aedb-26a111080c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback submitted successfully for run ID: 08d3f107-f66e-4ab5-97da-96aed14963ab\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Feedback score must be 0.0–1.0\n",
    "client.create_feedback(\n",
    "    run_id=run_id,\n",
    "    key=\"correctness\",\n",
    "    score=1.0,  # 1.0 = excellent, 0.0 = poor\n",
    "    value=\"Correct and relevant answer.\"\n",
    ")\n",
    "\n",
    "print(\"Feedback submitted successfully for run ID:\", run_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb98a7-31cb-4d26-bd57-9d317416ef10",
   "metadata": {},
   "source": [
    "## 6. Retrieving and Monitoring Runs\n",
    "\n",
    "You can fetch previously logged runs to monitor trends or analyze agent performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5b00d94-f827-4fc9-909b-d17e4ca34ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 08d3f107-f66e-4ab5-97da-96aed14963ab\n",
      "Name: OpenAIFunctionsAgentOutputParser\n",
      "Metadata: {'custom-metadata-key': 'custom-metadata-value', 'revision_id': 'my-revision-id', 'ls_run_depth': 2}\n",
      "Status: success\n",
      "--------------------------------------------------\n",
      "Run ID: 22b0a08f-fc7d-46df-8051-9f25ebcaa543\n",
      "Name: ChatOpenAI\n",
      "Metadata: {'custom-metadata-key': 'custom-metadata-value', 'revision_id': 'my-revision-id', 'ls_run_depth': 2}\n",
      "Status: success\n",
      "--------------------------------------------------\n",
      "Run ID: f56df947-6260-416e-b670-b651ae5a94a0\n",
      "Name: ChatPromptTemplate\n",
      "Metadata: {'custom-metadata-key': 'custom-metadata-value', 'revision_id': 'my-revision-id', 'ls_run_depth': 2}\n",
      "Status: success\n",
      "--------------------------------------------------\n",
      "Run ID: 96505265-d5e6-4629-a5a1-d00be3bde99f\n",
      "Name: RunnableLambda\n",
      "Metadata: {'custom-metadata-key': 'custom-metadata-value', 'revision_id': 'my-revision-id', 'ls_run_depth': 3}\n",
      "Status: success\n",
      "--------------------------------------------------\n",
      "Run ID: 3e50ca9c-8dac-4471-b92d-8c4a0e933d22\n",
      "Name: RunnableMap\n",
      "Metadata: {'custom-metadata-key': 'custom-metadata-value', 'revision_id': 'my-revision-id', 'ls_run_depth': 2}\n",
      "Status: success\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "recent_runs = list(client.list_runs(project_name=project_name, limit=5))\n",
    "\n",
    "for run in recent_runs:\n",
    "    print(f\"Run ID: {run.id}\")\n",
    "    print(f\"Name: {run.name}\")\n",
    "    print(f\"Metadata: {run.metadata}\")\n",
    "    print(f\"Status: {run.status}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d277a-0817-4d2a-97f0-141467a8d31b",
   "metadata": {},
   "source": [
    "## 7. Best Practices\n",
    "\n",
    "1. **Log every run** in production for auditing.\n",
    "2. Include **metadata** (model version, task, environment) for filtering.\n",
    "3. Regularly **evaluate agent responses** to ensure quality.\n",
    "4. Use ratings to **iterate on prompts, chains, and tools**.\n",
    "5. Monitor performance trends via **LangSmith dashboards**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c07a49-0591-4910-ab4f-e62004e6c81a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
