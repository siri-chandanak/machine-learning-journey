{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed0246e-a379-400c-8593-621bf76dda41",
   "metadata": {},
   "source": [
    "# PEFT Fine-Tuning with LoRA on IMDb Dataset\n",
    "\n",
    "**Objective:**  \n",
    "This document explains **Parameter-Efficient Fine-Tuning (PEFT)** using **LoRA (Low-Rank Adaptation)** on a **DistilBERT model** for **text classification**.  \n",
    "We use the **IMDb movie review dataset** for sentiment analysis (positive/negative classification).  \n",
    "\n",
    "**Key Concepts Covered:**\n",
    "\n",
    "- PEFT and LoRA for fine-tuning large models efficiently  \n",
    "- Targeting specific layers (query/value projections) in attention  \n",
    "- Manual PyTorch training loop (no `Trainer`, no `accelerate`)  \n",
    "- Evaluation of model performance  \n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Install Required Packages\n",
    "\n",
    "Required libraries:\n",
    "\n",
    "- `transformers`: Model and tokenizer  \n",
    "- `datasets`: IMDb dataset  \n",
    "- `peft`: LoRA fine-tuning  \n",
    "- `torch`: PyTorch training  \n",
    "\n",
    "**Install command:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0cc70d8-8fa6-47e3-b339-6502d656b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets peft torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f3dff3a-d240-4aa7-a841-ce8286086ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c099008-02a1-46fe-978c-08828c48ca59",
   "metadata": {},
   "source": [
    "## 2️⃣ Load IMDb Dataset\n",
    "\n",
    "We will load the dataset using **Hugging Face Datasets** library.  \n",
    "IMDb contains **50,000 movie reviews**, evenly split between positive and negative sentiment.  \n",
    "\n",
    "We will use a **small subset** for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65309c27-4e82-4334-9743-4e5d13ed6b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Inspect sample\n",
    "print(dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a826507-133e-4e4b-b499-1073333451f6",
   "metadata": {},
   "source": [
    "## 3️⃣ Tokenization\n",
    "\n",
    "We use **DistilBERT tokenizer** to convert text into token IDs.  \n",
    "\n",
    "- Pad or truncate each text to **max_length=128**  \n",
    "- Output includes `input_ids` and `attention_mask`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a07d4b58-4448-4651-920d-a75fec229bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17eaee67be9441b290f5ae3f0f59a2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_datasets = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130baf8-0d94-4ae5-a7d8-ed837b265d34",
   "metadata": {},
   "source": [
    "## 4️⃣ Create DataLoaders\n",
    "\n",
    "We will use **PyTorch DataLoaders** for batching the data:\n",
    "\n",
    "- Batch size = 16  \n",
    "- Shuffle the training set  \n",
    "- Select a smaller subset for faster experimentation  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69f83cf2-e0bb-4c08-ab2a-6cac5de85bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000)), batch_size=16)\n",
    "eval_loader = DataLoader(tokenized_datasets[\"test\"].shuffle(seed=42).select(range(500)), batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ed587-0186-4d6b-b009-22487b5bca42",
   "metadata": {},
   "source": [
    "## 5️⃣ Load Model and Inspect Layers\n",
    "\n",
    "We load **DistilBERT for sequence classification**.  \n",
    "\n",
    "- Inspect layer names to identify **query (`q_lin`) and value (`v_lin`) projection layers**  \n",
    "- LoRA will only modify these layers for **parameter-efficient fine-tuning**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "170c45b7-047e-4fe0-b199-419a0474bce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.transformer.layer.0.attention\n",
      "distilbert.transformer.layer.0.attention.dropout\n",
      "distilbert.transformer.layer.0.attention.q_lin\n",
      "distilbert.transformer.layer.0.attention.k_lin\n",
      "distilbert.transformer.layer.0.attention.v_lin\n",
      "distilbert.transformer.layer.0.attention.out_lin\n",
      "distilbert.transformer.layer.1.attention\n",
      "distilbert.transformer.layer.1.attention.dropout\n",
      "distilbert.transformer.layer.1.attention.q_lin\n",
      "distilbert.transformer.layer.1.attention.k_lin\n",
      "distilbert.transformer.layer.1.attention.v_lin\n",
      "distilbert.transformer.layer.1.attention.out_lin\n",
      "distilbert.transformer.layer.2.attention\n",
      "distilbert.transformer.layer.2.attention.dropout\n",
      "distilbert.transformer.layer.2.attention.q_lin\n",
      "distilbert.transformer.layer.2.attention.k_lin\n",
      "distilbert.transformer.layer.2.attention.v_lin\n",
      "distilbert.transformer.layer.2.attention.out_lin\n",
      "distilbert.transformer.layer.3.attention\n",
      "distilbert.transformer.layer.3.attention.dropout\n",
      "distilbert.transformer.layer.3.attention.q_lin\n",
      "distilbert.transformer.layer.3.attention.k_lin\n",
      "distilbert.transformer.layer.3.attention.v_lin\n",
      "distilbert.transformer.layer.3.attention.out_lin\n",
      "distilbert.transformer.layer.4.attention\n",
      "distilbert.transformer.layer.4.attention.dropout\n",
      "distilbert.transformer.layer.4.attention.q_lin\n",
      "distilbert.transformer.layer.4.attention.k_lin\n",
      "distilbert.transformer.layer.4.attention.v_lin\n",
      "distilbert.transformer.layer.4.attention.out_lin\n",
      "distilbert.transformer.layer.5.attention\n",
      "distilbert.transformer.layer.5.attention.dropout\n",
      "distilbert.transformer.layer.5.attention.q_lin\n",
      "distilbert.transformer.layer.5.attention.k_lin\n",
      "distilbert.transformer.layer.5.attention.v_lin\n",
      "distilbert.transformer.layer.5.attention.out_lin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Check attention layer names\n",
    "for name, _ in model.named_modules():\n",
    "    if \"attention\" in name:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3fb5fc-2e3a-4163-9565-bee920e953f3",
   "metadata": {},
   "source": [
    "## 6️⃣ Configure LoRA (PEFT)\n",
    "\n",
    "LoRA allows **low-rank adaptation** by injecting trainable matrices into **selected layers**, reducing the number of trainable parameters.  \n",
    "\n",
    "**Configuration parameters:**\n",
    "\n",
    "- `task_type`: `SEQ_CLS` (sequence classification)  \n",
    "- `r`: Rank of adaptation matrices (8 recommended)  \n",
    "- `lora_alpha`: Scaling factor  \n",
    "- `lora_dropout`: Dropout on LoRA layers  \n",
    "- `target_modules`: Layers to modify (`q_lin` and `v_lin`)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8fc14c3e-8ffc-4663-a19f-da537b8300f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]  # Correct modules for DistilBERT\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017fec7-43b2-44ff-8bc6-abc2b7fa4c69",
   "metadata": {},
   "source": [
    "## 7️⃣ Define Training Loop\n",
    "\n",
    "We will manually train the LoRA-adapted model:\n",
    "\n",
    "- Use **CrossEntropyLoss** for classification  \n",
    "- **Adam optimizer** with learning rate 5e-4  \n",
    "- Train for **1 epoch** (demo)  \n",
    "- Print **loss per epoch**\n",
    "  \n",
    "**Reason for using only 1 epoch:**\n",
    "\n",
    "- This notebook is intended as a **tutorial/demo**, so training on a small subset is sufficient to verify that the pipeline works.  \n",
    "- Training for only 1 epoch **reduces runtime** and GPU memory usage.  \n",
    "- For real-world applications or production models, you would train for **multiple epochs** until convergence.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "280c4152-05c6-49cd-80a3-6843360f04c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "Epoch 1 - Loss: 0.3633\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "print(\"training\")\n",
    "# Training\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f5754-60a9-4118-bcff-195d1e6ea098",
   "metadata": {},
   "source": [
    "## 8️⃣ Evaluate Model\n",
    "\n",
    "We calculate **accuracy** on the test subset:\n",
    "\n",
    "- Switch model to `eval()` mode  \n",
    "- Disable gradient calculation for faster evaluation  \n",
    "- Compare predictions with ground-truth labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "027bc236-1312-436a-ae05-b8365cc1200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8ed18-0693-4b0e-ad9c-8837a26c2088",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "\n",
    "- Loaded **IMDb dataset** and tokenized text  \n",
    "- Configured **LoRA for DistilBERT** with correct attention layers  \n",
    "- Trained model using **manual PyTorch loop**  \n",
    "- Evaluated test accuracy  \n",
    "\n",
    "**Advantages of PEFT + LoRA:**\n",
    "\n",
    "- Reduces number of trainable parameters  \n",
    "- Fine-tuning is faster and requires less GPU memory  \n",
    "- Easy to apply to large LLMs for downstream tasks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6875f5-2e2d-4fc1-8d00-231f4936a345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sparkenv)",
   "language": "python",
   "name": "sparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
