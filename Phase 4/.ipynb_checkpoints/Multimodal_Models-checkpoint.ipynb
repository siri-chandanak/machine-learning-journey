{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d144b50b-dbd7-4ba7-9ab8-b78f74446371",
   "metadata": {},
   "source": [
    "# Advanced Multimodal Models — CLIP & DALL·E\n",
    "\n",
    "This notebook explores **multimodal AI models**, focusing on **CLIP** and **DALL·E** — two foundational models that connect **language** and **vision**.\n",
    "\n",
    "---\n",
    "\n",
    "## What You’ll Learn\n",
    "1. What are multimodal models and why they matter  \n",
    "2. How CLIP connects text and images  \n",
    "3. How DALL·E generates images from text  \n",
    "4. How to use Hugging Face & OpenAI APIs for multimodal AI  \n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "- Python 3.8+\n",
    "- Install the following packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6c0949-20be-4636-834f-07a71d7a5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision transformers openai pillow matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc173a62-af5a-4597-8c98-258f416454cb",
   "metadata": {},
   "source": [
    "## What is Multimodal AI?\n",
    "\n",
    "**Multimodal AI** integrates multiple types of data — such as **text, image, audio, and video** — to enable richer understanding and generation.\n",
    "\n",
    "Examples:\n",
    "- **CLIP (Contrastive Language–Image Pretraining):** connects images and text.\n",
    "- **DALL·E:** generates images from text prompts.\n",
    "- **Whisper:** converts audio to text.\n",
    "- **BLIP / Flamingo / Gemini:** combine vision and language for reasoning.\n",
    "\n",
    "Multimodal systems are essential for:\n",
    "- Image captioning  \n",
    "- Visual question answering (VQA)  \n",
    "- Text-to-image generation  \n",
    "- AI-powered search and recommendation systems  \n",
    "\n",
    "## Understanding CLIP (Contrastive Language–Image Pretraining)\n",
    "\n",
    "CLIP, developed by OpenAI, learns to connect **text** and **images** by training on pairs of image–caption data.\n",
    "\n",
    "It learns embeddings for both modalities:\n",
    "- Image → visual embedding\n",
    "- Text → textual embedding  \n",
    "\n",
    "Then, it maximizes similarity for matching pairs and minimizes it for unrelated pairs.\n",
    "\n",
    "This makes CLIP useful for:\n",
    "- Zero-shot image classification  \n",
    "- Text–image similarity  \n",
    "- Image search engines  \n",
    "- Image captioning foundations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30d2a1-2436-422c-b300-41524bec5f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf59de1ab8542caac561ebcd709b9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksiri\\anaconda3\\envs\\diffusion_lab\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ksiri\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9b08723aa34eb5886a2717c9f42953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load an example image\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Define candidate texts\n",
    "texts = [\"a photo of a cat\", \"a photo of a dog\", \"a drawing of a cat\"]\n",
    "\n",
    "# Preprocess inputs\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Compute similarity scores\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # image-text similarity\n",
    "probs = logits_per_image.softmax(dim=1)      # normalize scores\n",
    "\n",
    "print(\"Texts:\", texts)\n",
    "print(\"Similarity probabilities:\", probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb0310-b197-4917-9908-d6b70ad02a4d",
   "metadata": {},
   "source": [
    "## Interpreting CLIP Results\n",
    "\n",
    "The model computes **similarity scores** between the image and each text.\n",
    "\n",
    "- The higher the score → the more the image matches the text.\n",
    "- CLIP can act as a **zero-shot classifier** — no task-specific training required.\n",
    "\n",
    "You can replace the texts with your own descriptions or labels (e.g., “a car”, “a person”, “a mountain”).\n",
    "\n",
    "## DALL·E: Text-to-Image Generation\n",
    "\n",
    "**DALL·E**, also by OpenAI, is a **generative model** that can create novel images from textual descriptions.\n",
    "\n",
    "It extends the idea of multimodal representation by learning **to generate pixels conditioned on text**.\n",
    "\n",
    "Example:  \n",
    "> Prompt: “An astronaut riding a horse in a photorealistic style.”\n",
    "\n",
    "DALL·E models include:\n",
    "- **DALL·E Mini / Craiyon:** lightweight public models  \n",
    "- **DALL·E 2 / DALL·E 3:** advanced OpenAI models with higher fidelity  \n",
    "\n",
    "We’ll generate images using the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79430d9-b9cc-4b65-b5e8-41e986c3d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from IPython.display import Image as IPyImage, display\n",
    "\n",
    "# Set your OpenAI API key (store securely)\n",
    "openai.api_key = \"your_openai_api_key_here\"\n",
    "\n",
    "# Define your text prompt\n",
    "prompt = \"A futuristic cityscape with flying cars and neon lights\"\n",
    "\n",
    "# Generate an image using DALL·E 3\n",
    "response = openai.images.generate(\n",
    "    model=\"gpt-image-1\",\n",
    "    prompt=prompt,\n",
    "    size=\"512x512\"\n",
    ")\n",
    "\n",
    "# Display generated image\n",
    "image_url = response.data[0].url\n",
    "display(IPyImage(url=image_url))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1d1e8-0265-4537-b792-d0e996645a42",
   "metadata": {},
   "source": [
    "## Tips for Better Prompts\n",
    "\n",
    "1. **Be descriptive:**  \n",
    "   “A sunset over the mountains” → “A vivid sunset over snowy mountains reflected in a lake.”\n",
    "\n",
    "2. **Specify style:**  \n",
    "   Add “in watercolor style,” “as a digital painting,” or “in Pixar 3D style.”\n",
    "\n",
    "3. **Add composition details:**  \n",
    "   “Wide-angle,” “portrait view,” “top-down perspective.”\n",
    "\n",
    "4. **Use modifiers:**  \n",
    "   “Realistic,” “abstract,” “fantasy,” “cinematic lighting,” “high resolution.”\n",
    "\n",
    "Prompt quality directly affects image quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89439de3-dfd1-491e-9a1e-09171c14d851",
   "metadata": {},
   "source": [
    "## CLIP + DALL·E: Text-Image Alignment\n",
    "\n",
    "Together, CLIP and DALL·E can:\n",
    "- Rank generated images by relevance using CLIP embeddings\n",
    "- Improve prompt adherence\n",
    "- Enable image retrieval, captioning, and generation in a unified pipeline\n",
    "\n",
    "**Workflow Example:**\n",
    "1. Generate 5 images using DALL·E  \n",
    "2. Use CLIP to compute similarity with the prompt  \n",
    "3. Select the top-ranked image  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51918b-a8d8-48ce-8fe7-4d739c15194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Suppose you have multiple image URLs (generated or downloaded)\n",
    "urls = [\n",
    "    \"https://example.com/image1.png\",\n",
    "    \"https://example.com/image2.png\",\n",
    "    \"https://example.com/image3.png\"\n",
    "]\n",
    "\n",
    "# Load CLIP\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load and process images\n",
    "images = [Image.open(requests.get(u, stream=True).raw) for u in urls]\n",
    "prompt = \"A futuristic cityscape with flying cars and neon lights\"\n",
    "inputs = clip_processor(text=[prompt], images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Compute similarity\n",
    "outputs = clip_model(**inputs)\n",
    "scores = outputs.logits_per_image.softmax(dim=1)\n",
    "print(\"Image relevance scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf7fbe-36c9-47ae-bb53-8455623da1ab",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- The fundamentals of **multimodal AI**\n",
    "- How **CLIP** links vision and language via embeddings\n",
    "- How **DALL·E** generates images from text\n",
    "- How to combine both models for search and generation tasks\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "- Try **BLIP** or **Flamingo** for captioning and visual question answering.  \n",
    "- Experiment with **OpenCLIP** and **Stable Diffusion** for local generation.  \n",
    "- Integrate CLIP with **vector search (FAISS, Pinecone)** for multimodal retrieval systems.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa869a8-387c-42fe-9968-3b3a15b26125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Diffusion Lab",
   "language": "python",
   "name": "diffusion_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
