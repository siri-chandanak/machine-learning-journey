{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f49cb82-786d-4cec-8b69-825bc5036ef5",
   "metadata": {},
   "source": [
    "# Advanced Reasoning in Generative AI\n",
    "This notebook covers **GraphRAG, Chain-of-Thought (CoT), and Tree-of-Thought (ToT)** reasoning techniques used in advanced AI systems.  \n",
    "\n",
    "We will explore:\n",
    "- Retrieval-Augmented Generation (RAG)\n",
    "- Chain-of-Thought (CoT) prompting\n",
    "- Tree-of-Thought (ToT) reasoning\n",
    "- Practical examples with Python\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "Advanced reasoning techniques allow AI systems, especially LLMs, to solve **complex, multi-step problems** beyond simple retrieval or single-step answers.\n",
    "\n",
    "1. **GraphRAG**: Combines **retrieval-based information** with reasoning over **knowledge graphs** for accurate response generation.\n",
    "2. **Chain-of-Thought (CoT)**: Forces the LLM to **think step-by-step**, improving reasoning and problem-solving.\n",
    "3. **Tree-of-Thought (ToT)**: Expands on CoT by **branching multiple reasoning paths**, evaluating them, and selecting the best one.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "We need the following Python libraries:\n",
    "- `transformers` (for LLM models)\n",
    "- `sentence-transformers` (for embedding and similarity search)\n",
    "- `faiss` (for vector search)\n",
    "- `networkx` (for graph reasoning, GraphRAG example)\n",
    "- `numpy` and `torch`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa3378e2-c359-4c05-b0b3-7777dc84a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (uncomment if needed)\n",
    "# !pip install transformers sentence-transformers faiss-cpu networkx torch numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b5edf-412b-4320-9ec1-b40a719d0735",
   "metadata": {},
   "source": [
    "## GraphRAG (Graph-based Retrieval-Augmented Generation)\n",
    "GraphRAG combines:\n",
    "- **Retrieval**: Search relevant documents or knowledge using embeddings.\n",
    "- **Graph Reasoning**: Connect concepts using nodes & edges in a graph.\n",
    "- **Generation**: Use LLM to generate answers using the retrieved info + graph reasoning.\n",
    "\n",
    "**Use case**: Question answering over a knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63d5d002-6df6-4af3-9fca-cff80b53d01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant nodes: ['Transformers']\n",
      "Connected nodes (context for generation): ['Deep Learning']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer:\n",
      " Question: What is used for Transformers in AI?\n",
      "Context: Deep Learning\n",
      "Answer: Deep Learning is a new approach to deep learning that uses deep learning to learn from data. It is a new approach to deep learning that uses deep learning to learn from data. It is a new approach to deep learning that uses deep learning to learn from\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Step 1: Build a small knowledge graph\n",
    "G = nx.Graph()\n",
    "G.add_edge(\"Python\", \"Machine Learning\")\n",
    "G.add_edge(\"Python\", \"Data Science\")\n",
    "G.add_edge(\"Machine Learning\", \"Deep Learning\")\n",
    "G.add_edge(\"Deep Learning\", \"Transformers\")\n",
    "\n",
    "# Step 2: Example query\n",
    "query = \"What is used for Transformers in AI?\"\n",
    "\n",
    "# Step 3: Simple retrieval: find nodes connected to query\n",
    "relevant_nodes = [node for node in G.nodes if node.lower() in query.lower()]\n",
    "neighbors = []\n",
    "for node in relevant_nodes:\n",
    "    neighbors.extend(list(G.neighbors(node)))\n",
    "\n",
    "print(\"Relevant nodes:\", relevant_nodes)\n",
    "print(\"Connected nodes (context for generation):\", neighbors)\n",
    "\n",
    "# Step 4: Use small LLM for demonstration\n",
    "model_name = \"gpt2\"  # lightweight model for demo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Generate answer\n",
    "input_text = f\"Question: {query}\\nContext: {', '.join(neighbors)}\\nAnswer:\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6afd2-7f46-473c-9921-137063818436",
   "metadata": {},
   "source": [
    "## Chain-of-Thought (CoT)\n",
    "CoT improves reasoning by prompting the model to **think step-by-step**.  \n",
    "\n",
    "**Example**:\n",
    "- Input: \"If there are 3 apples and I buy 2 more, how many apples do I have?\"\n",
    "- CoT prompt: \"Let's solve this step by step.\"\n",
    "- Output: Step-by-step reasoning followed by the answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b900b7fe-f99e-4ef0-9fe7-f088f9839f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT Answer:\n",
      " Question: If there are 3 apples and I buy 2 more, how many apples do I have?\n",
      "Let's solve this step by step:\n",
      "1. Pick the apples that are closest to the apple.\n",
      "2. Pick the apples that are closest to the apple.\n",
      "3. Pick the apples that are closest to the apple.\n",
      "4. Pick the apples that are closest to the\n"
     ]
    }
   ],
   "source": [
    "# Using GPT-2 for demonstration (replace with OpenAI API for production)\n",
    "question = \"If there are 3 apples and I buy 2 more, how many apples do I have?\"\n",
    "\n",
    "cot_prompt = f\"Question: {question}\\nLet's solve this step by step:\"\n",
    "\n",
    "inputs = tokenizer(cot_prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "answer_cot = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"CoT Answer:\\n\", answer_cot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e79b6c-a85d-4c74-a3eb-a81f0f233109",
   "metadata": {},
   "source": [
    "## Tree-of-Thought (ToT)\n",
    "ToT extends CoT by generating **multiple reasoning paths**, evaluating them, and selecting the most plausible answer.\n",
    "\n",
    "**Steps**:\n",
    "1. Generate multiple \"thought sequences\" (branches) from the same question.\n",
    "2. Score each sequence based on relevance/logic.\n",
    "3. Select the best reasoning path and produce the final answer.\n",
    "\n",
    "**Use case**: Complex reasoning, multi-step planning, AI agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a742e7b-d897-413d-8253-e823bb926c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Path 1:\n",
      "Path 1: Let's solve this step by step: 5 - 2 + 3 = 4 - 5 = 6 - 7 = 8 - 9 = 10 - 11 = 12 - 13 =\n",
      "\n",
      "Generated Path 2:\n",
      "Path 2: Stepwise calculation: 5 apples minus 2 apples, then add 3 apples: 1 apple = 1 apple + 2 apples = 1 apple + 2 apples = 1 apple + 2 apples\n",
      "\n",
      "Generated Path 3:\n",
      "Path 3: Calculation sequence: Start with 5, subtract 2, then add 3: Calculate sequence: Start with 5, subtract 2, then add 3: Calculate sequence: Start\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Toy example with multiple reasoning paths\n",
    "question = \"I have 5 apples, eat 2, and get 3 more. How many apples do I have?\"\n",
    "\n",
    "# Generate 3 reasoning paths\n",
    "paths = [\n",
    "    f\"Path 1: Let's solve this step by step: 5 - 2 + 3 =\",\n",
    "    f\"Path 2: Stepwise calculation: 5 apples minus 2 apples, then add 3 apples:\",\n",
    "    f\"Path 3: Calculation sequence: Start with 5, subtract 2, then add 3:\"\n",
    "]\n",
    "\n",
    "generated_paths = []\n",
    "for path in paths:\n",
    "    inputs = tokenizer(path, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "    generated_paths.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Print all generated paths\n",
    "for i, gpath in enumerate(generated_paths):\n",
    "    print(f\"Generated Path {i+1}:\\n{gpath}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47c074-71b5-4064-8b8c-316e08dff587",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- **GraphRAG**: Combines retrieval with graph reasoning for contextual answers.  \n",
    "- **Chain-of-Thought (CoT)**: Forces step-by-step reasoning for complex questions.  \n",
    "- **Tree-of-Thought (ToT)**: Explores multiple reasoning paths and selects the best one.  \n",
    "\n",
    "These techniques are essential for **advanced AI applications**, including multi-step question answering, planning, and agentic AI systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e86c0f-380b-43ea-9e21-036055b55dd9",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "- Advanced reasoning enables **interpretability, reliability, and control** in large language models.\n",
    "- Integrating GraphRAG, CoT, and ToT enhances **multi-step reasoning and contextual accuracy**.\n",
    "- These techniques are foundational for **intelligent agents**, **automated tutors**, **research assistants**, and **decision-support systems**.\n",
    "- Future AI systems will rely on these reasoning methods to perform **explainable, adaptive, and goal-driven** tasks.\n",
    "\n",
    "### Next Steps\n",
    "If you’re continuing your Phase 4 journey:\n",
    "1. Experiment with **real LLMs (LLaMA 3, GPT-4, Claude 3)** for reasoning benchmarks.  \n",
    "2. Implement **GraphRAG pipelines** with LangChain and Neo4j.  \n",
    "3. Build **ToT-based agents** for planning or creative writing tasks.  \n",
    "4. Combine **reasoning + retrieval** to create domain-specific expert systems.\n",
    "\n",
    "> “Reasoning is not about knowing the answer — it’s about discovering the path to it.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f09372-4d06-4f6b-a721-f0a6a50d4707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
