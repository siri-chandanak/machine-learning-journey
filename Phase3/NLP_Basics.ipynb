{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47b61bb-2919-4b92-a876-6d38e7594e41",
   "metadata": {},
   "source": [
    "# NLP Basics: Text Preprocessing, Vectorization, and Text Similarity\n",
    "\n",
    "**Objective:**  \n",
    "This notebook introduces fundamental Natural Language Processing (NLP) concepts, including text preprocessing, Bag of Words (BoW), TF-IDF vectorization, and text similarity measures.  \n",
    "By the end of this notebook, you will be able to:\n",
    "- Preprocess raw text for NLP tasks\n",
    "- Convert text into numerical representations\n",
    "- Compute similarity between text documents\n",
    "\n",
    "## What is NLP?\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of AI and linguistics that focuses on enabling computers to understand, interpret, and generate human language.\n",
    "\n",
    "**Key Tasks in NLP:**\n",
    "- Text Preprocessing\n",
    "- Text Representation (BoW, TF-IDF, Word Embeddings)\n",
    "- Text Classification\n",
    "- Named Entity Recognition (NER)\n",
    "- Sentiment Analysis\n",
    "- Machine Translation\n",
    "- Question Answering\n",
    "\n",
    "## Required Python Libraries\n",
    "\n",
    "We will use the following Python libraries:\n",
    "\n",
    "- `nltk` for tokenization, stopwords, stemming, and lemmatization\n",
    "- `scikit-learn` for vectorization (BoW, TF-IDF) and similarity computation\n",
    "- `numpy` for numerical operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a68b814-bf06-4c74-b259-b1e5c53e85e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ksiri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ksiri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ksiri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install libraries if not already installed\n",
    "#!pip install nltk scikit-learn\n",
    "\n",
    "# Import libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df003f20-2cb6-4738-abd9-bf6554f11e0a",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Text preprocessing is the first step in NLP. It involves cleaning and normalizing text to make it suitable for analysis or model input.\n",
    "\n",
    "**Common Preprocessing Steps:**\n",
    "1. **Lowercasing** – Convert all text to lowercase to ensure uniformity.\n",
    "2. **Tokenization** – Split text into words or sentences.\n",
    "3. **Stopwords Removal** – Remove common words (like \"the\", \"is\") that do not carry much meaning.\n",
    "4. **Stemming** – Reduce words to their root form (e.g., \"running\" → \"run\").\n",
    "5. **Lemmatization** – Reduce words to their dictionary form (e.g., \"better\" → \"good\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "696130ca-eccd-41dc-a69f-0fe3d61712dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['natural', 'language', 'processing', 'is', 'amazing', '!', 'nlp', 'helps', 'machines', 'understand', 'text', '.']\n",
      "Filtered Tokens: ['natural', 'language', 'processing', 'amazing', 'nlp', 'helps', 'machines', 'understand', 'text']\n",
      "Stemmed Tokens: ['natur', 'languag', 'process', 'amaz', 'nlp', 'help', 'machin', 'understand', 'text']\n",
      "Lemmatized Tokens: ['natural', 'language', 'processing', 'amazing', 'nlp', 'help', 'machine', 'understand', 'text']\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Natural Language Processing is amazing! NLP helps machines understand text.\"\n",
    "\n",
    "# Lowercase & Tokenize\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "print(\"Filtered Tokens:\", filtered_tokens)\n",
    "\n",
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "stemmed_tokens = [ps.stem(w) for w in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237e7ed5-fbac-42f1-bcf5-08c890917d68",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)\n",
    "\n",
    "**Definition:**  \n",
    "BoW is a method to represent text as a fixed-length vector of word counts. It ignores grammar and word order but captures frequency information.\n",
    "\n",
    "**Steps:**\n",
    "1. Create a vocabulary of all unique words.\n",
    "2. Count occurrences of each word in the document.\n",
    "3. Represent each document as a vector of word counts.\n",
    "\n",
    "**Example:**  \n",
    "Text: [\"I love NLP\", \"NLP is fun\"] → Vocabulary: [\"I\", \"love\", \"NLP\", \"is\", \"fun\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6dfa92d-5419-4fb1-b017-b7a7bc2e9787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Feature Names: ['data' 'fascinating' 'helps' 'is' 'language' 'learning' 'love' 'machine'\n",
      " 'natural' 'processing' 'understand']\n",
      "BoW Matrix:\n",
      " [[0 0 0 0 0 1 1 1 0 0 0]\n",
      " [0 1 0 1 1 0 0 0 1 1 0]\n",
      " [1 0 1 0 0 1 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"I love machine learning.\",\n",
    "    \"Natural Language Processing is fascinating.\",\n",
    "    \"Machine learning helps understand data.\"\n",
    "]\n",
    "\n",
    "# Create BoW representation\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"BoW Feature Names:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Matrix:\\n\", X_bow.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90f1f3-308b-44da-b56b-9ecbbf2bd7fd",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency – Inverse Document Frequency)\n",
    "\n",
    "**Definition:**  \n",
    "TF-IDF is a method to represent text while reducing the weight of common words and increasing the weight of rare but important words.\n",
    "\n",
    "**Formulas:**\n",
    "- **TF (Term Frequency):** Frequency of a word in a document.\n",
    "- **IDF (Inverse Document Frequency):** log(N / df), where N = total documents, df = number of docs containing the word.\n",
    "\n",
    "**Benefits:**  \n",
    "- Reduces impact of frequent words (like \"the\")\n",
    "- Highlights important terms for the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0318181-f010-4bf7-8493-1302a1623771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Feature Names: ['data' 'fascinating' 'helps' 'is' 'language' 'learning' 'love' 'machine'\n",
      " 'natural' 'processing' 'understand']\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.         0.         0.         0.         0.51785612\n",
      "  0.68091856 0.51785612 0.         0.         0.        ]\n",
      " [0.         0.4472136  0.         0.4472136  0.4472136  0.\n",
      "  0.         0.         0.4472136  0.4472136  0.        ]\n",
      " [0.49047908 0.         0.49047908 0.         0.         0.37302199\n",
      "  0.         0.37302199 0.         0.         0.49047908]]\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"TF-IDF Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", X_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb56ec-58e2-457f-87ce-00a8358d9e57",
   "metadata": {},
   "source": [
    "## Text Similarity\n",
    "\n",
    "After converting text into vectors (BoW or TF-IDF), we can compute similarity between documents.\n",
    "\n",
    "**Common Similarity Measures:**\n",
    "- Cosine Similarity\n",
    "- Euclidean Distance\n",
    "\n",
    "**Use Case:** Finding similar documents, clustering, information retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a00735d-9148-46c4-8359-980dbeb080ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      " [[1.         0.         0.38634343]\n",
      " [0.         1.         0.        ]\n",
      " [0.38634343 0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity between documents\n",
    "similarity_matrix = cosine_similarity(X_tfidf)\n",
    "print(\"Cosine Similarity Matrix:\\n\", similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1570a27-250e-4714-b254-858b5526b61d",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- Basic text preprocessing: tokenization, stopwords removal, stemming, lemmatization.\n",
    "- Representing text numerically using Bag of Words and TF-IDF.\n",
    "- Measuring text similarity using cosine similarity.\n",
    "\n",
    "These are foundational NLP skills required before moving to **word embeddings, sequence models, and transformers**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd6438-83a1-47fe-acba-c53c169b1dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
