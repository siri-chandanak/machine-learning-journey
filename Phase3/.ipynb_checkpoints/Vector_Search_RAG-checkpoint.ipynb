{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c8fe9c-55fb-45ff-9e83-e3b35cba463f",
   "metadata": {},
   "source": [
    "# Vector Search & Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "This notebook covers:\n",
    "- How to convert documents into vector embeddings\n",
    "- Building vector search indices with FAISS (or Pinecone/Chroma)\n",
    "- Querying vector databases to retrieve relevant documents\n",
    "- Integrating embeddings with LLMs for **RAG pipelines**\n",
    "\n",
    "By the end, you'll be able to build a **QA chatbot or AI-powered search engine** using embeddings + LLMs.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You should already be familiar with:\n",
    "- NLP basics: tokenization, preprocessing, embeddings\n",
    "- Transformers and LLMs (BERT, GPT, T5)\n",
    "- Python and PyTorch/TensorFlow basics\n",
    "\n",
    "We will use:\n",
    "- Hugging Face Transformers for embedding models\n",
    "- FAISS / Pinecone / Chroma for vector search\n",
    "- OpenAI GPT-3 or any other LLM for generating answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36dbcdfd-e29d-46da-b537-6399f7844e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ksiri\\anaconda3\\envs\\sparkenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu sentence-transformers transformers pinecone-client chromadb --quiet\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "# Optional: Pinecone / Chroma\n",
    "# import pinecone\n",
    "# import chromadb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087e536-6178-4ba6-a9b7-828b67a75ec2",
   "metadata": {},
   "source": [
    "# Step 1: Prepare Data\n",
    "\n",
    "We need a set of documents to build our vector database.\n",
    "\n",
    "For example, we can use:\n",
    "- Text articles\n",
    "- FAQs\n",
    "- Wikipedia pages\n",
    "- Custom datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc05aa7-a44c-4aea-b00a-cea31cdb7df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Python is a high-level programming language designed for readability and simplicity.\",\n",
    "    \"Machine Learning enables computers to learn from data without being explicitly programmed.\",\n",
    "    \"Transformers are neural network architectures that rely on self-attention mechanisms.\",\n",
    "    \"FAISS is a library for efficient similarity search and clustering of dense vectors.\",\n",
    "    \"OpenAI GPT-3 is a large language model capable of natural language understanding and generation.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d00a6-0954-45b3-8951-05d051afa00e",
   "metadata": {},
   "source": [
    "# Step 2: Convert Documents to Vector Embeddings\n",
    "\n",
    "We use a pre-trained sentence transformer to generate embeddings for each document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e090b3-511d-485a-b077-a516aed6d145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2a28e350384fdb9eb8a62ce608fcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksiri\\anaconda3\\envs\\sparkenv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ksiri\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210826319ffb4305968741f5d09d5846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d460a2016cb4be1ad447b0435e7f973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5046b659dc4c719670f00cdc0d6afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269d385df2f04d04b6959e31fca38718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63ba3c4d81e4ffc83018f878ca22978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (5, 384)\n"
     ]
    }
   ],
   "source": [
    "# Load Sentence Transformer Model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')  # lightweight model for embeddings\n",
    "\n",
    "# Encode documents\n",
    "doc_embeddings = embedder.encode(documents)\n",
    "print(\"Embeddings shape:\", doc_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8892d7-d7a8-476c-a26a-be25d780599d",
   "metadata": {},
   "source": [
    "# Step 3: Build FAISS Index\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) allows fast vector search using nearest neighbor search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f6dfbcd-9a2c-4a74-9b7d-ec2662251092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in index: 5\n"
     ]
    }
   ],
   "source": [
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
    "index.add(np.array(doc_embeddings))\n",
    "print(f\"Number of vectors in index: {index.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127f1c1d-c246-474f-92fb-7cb62650090b",
   "metadata": {},
   "source": [
    "# Step 4: Query the Vector Database\n",
    "\n",
    "We can now query the FAISS index to retrieve documents similar to a given query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2278e7f3-24cc-45d3-b6e2-954eb94ffd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: OpenAI GPT-3 is a large language model capable of natural language understanding and generation. (Distance: 0.9818)\n",
      "Result 2: Machine Learning enables computers to learn from data without being explicitly programmed. (Distance: 1.3482)\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about natural language processing in AI.\"\n",
    "query_embedding = embedder.encode([query])\n",
    "\n",
    "# Search for top-2 closest documents\n",
    "k = 2\n",
    "distances, indices = index.search(np.array(query_embedding), k)\n",
    "\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Result {i+1}: {documents[idx]} (Distance: {distances[0][i]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c86b9-7942-4b48-921e-aaa8f2b5eb36",
   "metadata": {},
   "source": [
    "# Step 5: Integrate with LLM (RAG Pipeline)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) uses:\n",
    "1. **Retriever:** Find relevant documents via vector search\n",
    "2. **Generator:** Use LLM to generate answers based on retrieved documents\n",
    "\n",
    "Workflow:\n",
    "- User asks a question\n",
    "- Convert question to embedding\n",
    "- Retrieve top relevant documents\n",
    "- Pass documents + question to GPT-3 (or another LLM) for answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f07a120-93c8-46d9-9880-6bb077599795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context passed to LLM for answer generation:\n",
      "OpenAI GPT-3 is a large language model capable of natural language understanding and generation. Machine Learning enables computers to learn from data without being explicitly programmed.\n"
     ]
    }
   ],
   "source": [
    "# Example using OpenAI GPT (pseudo-code)\n",
    "# !pip install openai\n",
    "# import openai\n",
    "\n",
    "retrieved_docs = [documents[idx] for idx in indices[0]]\n",
    "context = \" \".join(retrieved_docs)\n",
    "question = \"Explain AI transformers.\"\n",
    "\n",
    "# Pseudo-code for GPT query\n",
    "# response = openai.Completion.create(\n",
    "#     engine=\"text-davinci-003\",\n",
    "#     prompt=f\"Answer the question based on context:\\nContext: {context}\\nQuestion: {question}\",\n",
    "#     max_tokens=150\n",
    "# )\n",
    "# print(response.choices[0].text.strip())\n",
    "\n",
    "print(\"Context passed to LLM for answer generation:\")\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e81882-f5a4-4595-8b36-a2124fc7fdc2",
   "metadata": {},
   "source": [
    "# Step 6: Hybrid Search (Keyword + Vector)\n",
    "\n",
    "Sometimes, combining:\n",
    "- **Keyword search:** traditional TF-IDF/BM25\n",
    "- **Vector search:** semantic similarity\n",
    "\n",
    "Improves retrieval quality for RAG pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f5ba13-33c5-4fda-83b9-b54820e5bf3c",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- How to generate embeddings for documents using Sentence Transformers\n",
    "- How to build a **FAISS index** for vector similarity search\n",
    "- How to query the index to retrieve relevant documents\n",
    "- How to integrate retrieval with LLMs to form **RAG pipelines**\n",
    "- Optional: Hybrid search strategies\n",
    "\n",
    "Next Steps:\n",
    "- Experiment with larger datasets\n",
    "- Use Pinecone or Chroma for cloud-hosted vector search\n",
    "- Integrate with fine-tuned LLMs for production-grade RAG applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf424f2f-985d-40db-a44a-333ff80c299c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sparkenv)",
   "language": "python",
   "name": "sparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
